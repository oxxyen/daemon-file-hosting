#!/usr/bin/env python3
"""
ULTIMATE C/C++ STATIC ANALYZER - Senior Level
–ú–æ—â–Ω–µ–π—à–∏–π —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤
"""
import argparse
import os
import re
import ast
import subprocess
import threading
import queue
from pathlib import Path
from dataclasses import dataclass, field
from typing import List, Dict, Set, Tuple, Optional, Any
import json
import time
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed, ProcessPoolExecutor
from collections import defaultdict, deque
import hashlib
import pickle
import statistics
from enum import Enum
import platform
import psutil
from datetime import datetime
import multiprocessing as mp
from contextlib import contextmanager
import functools
import inspect

# –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
try:
    import clang
    from clang.cindex import Index, Config, TranslationUnit, Cursor, CursorKind
    CLANG_AVAILABLE = True
except ImportError:
    CLANG_AVAILABLE = False

try:
    import radon
    from radon.complexity import cc_visit, cc_rank
    from radon.metrics import h_visit, mi_visit
    from radon.raw import analyze
    RADON_AVAILABLE = True
except ImportError:
    RADON_AVAILABLE = False

try:
    import lizard
    LIZARD_AVAILABLE = True
except ImportError:
    LIZARD_AVAILABLE = False

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
class ColoredFormatter(logging.Formatter):
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ª–æ–≥–æ–≤ —Å —Ü–≤–µ—Ç–∞–º–∏"""
    COLORS = {
        'DEBUG': '\033[36m',
        'INFO': '\033[32m',
        'WARNING': '\033[33m',
        'ERROR': '\033[31m',
        'CRITICAL': '\033[41m',
        'RESET': '\033[0m'
    }
    
    def format(self, record):
        log_message = super().format(record)
        return f"{self.COLORS.get(record.levelname, '')}{log_message}{self.COLORS['RESET']}"

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞
def setup_logging(verbose=False):
    logger = logging.getLogger()
    logger.setLevel(logging.DEBUG if verbose else logging.INFO)
    
    # File handler
    file_handler = logging.FileHandler('ultimate_analyzer.log', encoding='utf-8')
    file_handler.setFormatter(logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    ))
    
    # Console handler with colors
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(ColoredFormatter(
        '%(asctime)s - %(levelname)s - %(message)s'
    ))
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)

class IssueSeverity(Enum):
    CRITICAL = 5
    HIGH = 4
    MEDIUM = 3
    LOW = 2
    INFO = 1

class AnalysisCategory(Enum):
    MEMORY_SAFETY = "memory_safety"
    SECURITY = "security"
    PERFORMANCE = "performance"
    CODE_QUALITY = "code_quality"
    CONCURRENCY = "concurrency"

@dataclass
class CodeIssue:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º –≤ –∫–æ–¥–µ"""
    id: str
    file_path: str
    line_number: int
    column: int
    issue_type: str
    category: AnalysisCategory
    severity: IssueSeverity
    description: str
    code_snippet: str
    confidence: float
    suggestions: List[str] = field(default_factory=list)
    cwe: List[str] = field(default_factory=list)
    metrics: Dict[str, Any] = field(default_factory=dict)
    context: Dict[str, Any] = field(default_factory=dict)

@dataclass
class FileMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∞–π–ª–∞"""
    complexity: float = 0.0
    maintainability: float = 0.0
    lines_of_code: int = 0
    comment_ratio: float = 0.0
    function_count: int = 0
    issue_density: float = 0.0
    memory_risk_score: float = 0.0

class AdvancedPatternEngine:
    """–î–≤–∏–∂–æ–∫ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —Å –º–∞—à–∏–Ω–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º"""
    
    def __init__(self):
        self.patterns = self._load_advanced_patterns()
        self.learned_patterns = self._load_learned_patterns()
        
    def _load_advanced_patterns(self) -> Dict[str, List[Dict]]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
        return {
            'memory_leak': [
                {
                    'pattern': r'(?:(?:malloc|calloc|realloc|strdup)\s*\([^)]+\)|new\s+[^{];]*)(?!.*(?:free|delete)\s*\(\s*\w+\s*\))',
                    'context': {'requires_allocation': True, 'requires_free': True},
                    'weight': 0.9
                },
                {
                    'pattern': r'(\w+)\s*=\s*(?:malloc|calloc|realloc|strdup|new).*?(?:(?=return|\})|$)',
                    'context': {'track_variable': True},
                    'weight': 0.8
                }
            ],
            'double_free': [
                {
                    'pattern': r'free\s*\(\s*(\w+)\s*\).*?free\s*\(\s*\1\s*\)',
                    'context': {'dangerous': True},
                    'weight': 0.95
                }
            ],
            'use_after_free': [
                {
                    'pattern': r'free\s*\(\s*(\w+)\s*\).*?\1\s*[=!<>\[\]]',
                    'context': {'critical': True},
                    'weight': 0.97
                }
            ],
            'buffer_overflow': [
                {
                    'pattern': r'(strcpy|strcat|sprintf|gets|scanf)\s*\(',
                    'context': {'security_risk': True},
                    'weight': 0.85
                },
                {
                    'pattern': r'(\w+)\s*\[.*?\]\s*=\s*[^;]+;.*?\1\s*\[.*?\].*?=',
                    'context': {'bounds_check': False},
                    'weight': 0.7
                }
            ],
            'null_pointer': [
                {
                    'pattern': r'(\w+)\s*=\s*(?:NULL|nullptr).*?\1\s*->',
                    'context': {'null_dereference': True},
                    'weight': 0.96
                }
            ],
            'concurrency_issues': [
                {
                    'pattern': r'pthread_mutex_lock.*?pthread_mutex_unlock',
                    'context': {'requires_pairing': True},
                    'weight': 0.75
                },
                {
                    'pattern': r'(\w+)\s*=\s*\w+\s*\+\s*\w+.*?(?=pthread_mutex_lock)',
                    'context': {'race_condition': True},
                    'weight': 0.8
                }
            ]
        }
    
    def _load_learned_patterns(self) -> Dict:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ (–∑–∞–≥–ª—É—à–∫–∞ –¥–ª—è ML –º–æ–¥–µ–ª–∏)"""
        # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–¥–µ—Å—å –±—ã–ª–∞ –±—ã –∑–∞–≥—Ä—É–∑–∫–∞ ML –º–æ–¥–µ–ª–∏
        return {}
    
    def analyze_with_context(self, code: str, file_context: Dict) -> List[Dict]:
        """–ê–Ω–∞–ª–∏–∑ —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Ñ–∞–π–ª–∞"""
        issues = []
        
        for category, patterns in self.patterns.items():
            for pattern_info in patterns:
                matches = re.finditer(pattern_info['pattern'], code, re.MULTILINE | re.DOTALL)
                for match in matches:
                    issue = {
                        'type': category,
                        'match': match.group(),
                        'position': match.span(),
                        'weight': pattern_info['weight'],
                        'context': pattern_info['context']
                    }
                    issues.append(issue)
        
        return self._apply_ml_heuristics(issues, code, file_context)
    
    def _apply_ml_heuristics(self, issues: List[Dict], code: str, context: Dict) -> List[Dict]:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ ML —ç–≤—Ä–∏—Å—Ç–∏–∫ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è ML-—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        filtered_issues = []
        
        for issue in issues:
            # –≠–≤—Ä–∏—Å—Ç–∏–∫–∞: –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å —à–∞–±–ª–æ–Ω–Ω—ã–µ –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è –≤ –¥–µ—Å—Ç—Ä—É–∫—Ç–æ—Ä–∞—Ö
            if 'free' in issue['match'] and '~' in code:
                issue['weight'] *= 0.5
            
            # –≠–≤—Ä–∏—Å—Ç–∏–∫–∞: –ø–æ–≤—ã—Å–∏—Ç—å –≤–µ—Å –≤ —Å–ª–æ–∂–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏—è—Ö
            if context.get('high_complexity', False):
                issue['weight'] *= 1.2
            
            if issue['weight'] > 0.6:  # –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
                filtered_issues.append(issue)
        
        return filtered_issues

class ControlFlowAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ø–æ—Ç–æ–∫–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–ª—è —Å–ª–æ–∂–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
    
    def __init__(self):
        self.cfg = {}  # Control Flow Graph
        self.data_flow = {}
        
    def build_control_flow_graph(self, code: str) -> Dict:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –ø–æ—Ç–æ–∫–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è"""
        lines = code.split('\n')
        cfg = {}
        current_block = []
        block_id = 0
        
        for i, line in enumerate(lines):
            line = line.strip()
            
            # –ù–∞—á–∞–ª–æ –Ω–æ–≤–æ–≥–æ –±–∞–∑–æ–≤–æ–≥–æ –±–ª–æ–∫–∞
            if self._is_control_flow_statement(line) and current_block:
                cfg[block_id] = current_block.copy()
                current_block = []
                block_id += 1
            
            current_block.append((i + 1, line))
            
            # –ö–æ–Ω–µ—Ü —Ñ—É–Ω–∫—Ü–∏–∏ –∏–ª–∏ –æ–±–ª–∞—Å—Ç–∏ –≤–∏–¥–∏–º–æ—Å—Ç–∏
            if line == '}' and current_block:
                cfg[block_id] = current_block.copy()
                current_block = []
                block_id += 1
        
        if current_block:
            cfg[block_id] = current_block
        
        return cfg
    
    def _is_control_flow_statement(self, line: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–æ—Ç–æ–∫–æ–º"""
        patterns = [
            r'^if\s*\(', r'^else', r'^for\s*\(', r'^while\s*\(', 
            r'^switch\s*\(', r'^case\s+', r'^return\s+',
            r'^break\s*;', r'^continue\s*;', r'^goto\s+'
        ]
        return any(re.match(pattern, line) for pattern in patterns)
    
    def analyze_data_flow(self, cfg: Dict) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –ø–æ—Ç–æ–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
        variable_states = {}
        
        for block_id, instructions in cfg.items():
            for line_num, instruction in instructions:
                self._update_variable_states(variable_states, instruction, line_num)
        
        return variable_states
    
    def _update_variable_states(self, states: Dict, instruction: str, line_num: int):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏–π –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö"""
        # –ê–Ω–∞–ª–∏–∑ –≤—ã–¥–µ–ª–µ–Ω–∏—è –ø–∞–º—è—Ç–∏
        alloc_matches = re.finditer(r'(\w+)\s*=\s*(malloc|calloc|realloc|new)\s*\(', instruction)
        for match in alloc_matches:
            var_name = match.group(1)
            states[var_name] = {
                'allocated': True,
                'freed': False,
                'allocation_line': line_num,
                'last_used': line_num
            }
        
        # –ê–Ω–∞–ª–∏–∑ –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è –ø–∞–º—è—Ç–∏
        free_matches = re.finditer(r'(free|delete)\s*\(\s*(\w+)\s*\)', instruction)
        for match in free_matches:
            var_name = match.group(2)
            if var_name in states:
                states[var_name]['freed'] = True
                states[var_name]['free_line'] = line_num
        
        # –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        use_matches = re.finditer(r'(\w+)\s*[=!<>+\-*/]', instruction)
        for match in use_matches:
            var_name = match.group(1)
            if var_name in states:
                states[var_name]['last_used'] = line_num

class SemanticAnalyzer:
    """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å –≥–ª—É–±–æ–∫–∏–º –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º –∫–æ–¥–∞"""
    
    def __init__(self):
        self.symbol_table = {}
        self.type_system = {}
        self.function_prototypes = {}
        
    def analyze_symbols(self, code: str) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ —Å–∏–º–≤–æ–ª–æ–≤ –∏ –∏—Ö —Ç–∏–ø–æ–≤"""
        symbols = {}
        
        # –ü–æ–∏—Å–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏–π –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        var_patterns = [
            r'(int|char|float|double|void|long|short|unsigned)\s+(\w+)\s*(?:=\s*[^;]+)?;',
            r'(struct|union|enum)\s+(\w+)\s*\{[^}]+\}\s*(\w*);',
            r'(\w+)\s*\*\s*(\w+)\s*;'
        ]
        
        for pattern in var_patterns:
            matches = re.finditer(pattern, code)
            for match in matches:
                var_type = match.group(1)
                var_name = match.group(2) if match.groups() > 1 else match.group(1)
                symbols[var_name] = {
                    'type': var_type,
                    'size': self._estimate_size(var_type),
                    'scope': 'global' if ';' in code.split('{')[0] else 'local'
                }
        
        return symbols
    
    def _estimate_size(self, type_name: str) -> int:
        """–û—Ü–µ–Ω–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ —Ç–∏–ø–∞"""
        sizes = {
            'char': 1, 'int': 4, 'float': 4, 'double': 8,
            'long': 8, 'short': 2, 'void*': 8
        }
        return sizes.get(type_name, 4)
    
    def analyze_function_complexity(self, code: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Ñ—É–Ω–∫—Ü–∏–π"""
        if not RADON_AVAILABLE:
            return {}
        
        try:
            complexity_results = cc_visit(code)
            metrics = {}
            
            for func in complexity_results:
                metrics[func.name] = {
                    'complexity': func.complexity,
                    'rank': cc_rank(func.complexity),
                    'lines': func.endline - func.lineno + 1
                }
            
            return metrics
        except Exception as e:
            logging.warning(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏: {e}")
            return {}

class AdvancedMemoryAnalyzer:
    """–ú–æ—â–Ω–µ–π—à–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ø–∞–º—è—Ç–∏ –∏ –ø—Ä–æ–±–ª–µ–º –≤ C/C++ –∫–æ–¥–µ"""
    
    def __init__(self, project_path: str, config: Dict = None):
        self.project_path = Path(project_path)
        self.config = config or {}
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self.pattern_engine = AdvancedPatternEngine()
        self.control_flow_analyzer = ControlFlowAnalyzer()
        self.semantic_analyzer = SemanticAnalyzer()
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        self.issues: List[CodeIssue] = []
        self.metrics: Dict[str, FileMetrics] = {}
        self.stats = {
            'files_analyzed': 0,
            'total_lines': 0,
            'analysis_time': 0,
            'memory_used': 0
        }
        
        # –ö—ç—à –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞
        self._cache = {}
        self._cache_file = self.project_path / '.analyzer_cache'
        self._load_cache()
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–Ω–æ–≥–æ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–Ω–æ—Å—Ç–∏
        self.max_processes = min(mp.cpu_count(), self.config.get('max_processes', 8))
        
        setup_logging(self.config.get('verbose', False))
        self.logger = logging.getLogger(__name__)

    def _load_cache(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫—ç—à–∞ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
        try:
            if self._cache_file.exists():
                with open(self._cache_file, 'rb') as f:
                    self._cache = pickle.load(f)
                self.logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω –∫—ç—à –∞–Ω–∞–ª–∏–∑–∞ ({len(self._cache)} –∑–∞–ø–∏—Å–µ–π)")
        except Exception as e:
            self.logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫—ç—à: {e}")

    def _save_cache(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫—ç—à–∞ –∞–Ω–∞–ª–∏–∑–∞"""
        try:
            with open(self._cache_file, 'wb') as f:
                pickle.dump(self._cache, f)
        except Exception as e:
            self.logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫—ç—à: {e}")

    def find_source_files(self) -> List[Path]:
        """–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –∏—Å—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        source_files = []
        ignore_patterns = {
            '*.o', '*.so', '*.a', '*.dll', '*.exe', '*.bin',
            'build/*', 'cmake-build-*', '.git/*', '*.gitignore'
        }
        
        extensions = {'.c', '.cpp', '.cc', '.cxx', '.h', '.hpp', '.hxx'}
        
        for ext in extensions:
            for file_path in self.project_path.rglob(f'*{ext}'):
                if not any(file_path.match(pattern) for pattern in ignore_patterns):
                    source_files.append(file_path)
        
        self.logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(source_files)} –∏—Å—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤")
        return source_files

    def calculate_file_hash(self, file_path: Path) -> str:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ö–µ—à–∞ —Ñ–∞–π–ª–∞ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è"""
        content = file_path.read_bytes()
        return hashlib.md5(content).hexdigest()

    def analyze_file_advanced(self, file_path: Path) -> Tuple[List[CodeIssue], FileMetrics]:
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞"""
        file_hash = self.calculate_file_hash(file_path)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
        if file_hash in self._cache and not self.config.get('force_reanalyze', False):
            return self._cache[file_hash]
        
        try:
            content = file_path.read_text(encoding='utf-8', errors='ignore')
            issues = []
            metrics = FileMetrics()
            
            # –ú–Ω–æ–≥–æ–º–æ–¥—É–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
            analysis_methods = [
                self._pattern_based_analysis,
                self._control_flow_analysis,
                self._semantic_analysis,
                self._metrics_analysis,
                self._security_analysis,
                self._performance_analysis
            ]
            
            for method in analysis_methods:
                try:
                    method_issues, method_metrics = method(file_path, content)
                    issues.extend(method_issues)
                    self._merge_metrics(metrics, method_metrics)
                except Exception as e:
                    self.logger.error(f"–û—à–∏–±–∫–∞ –≤ {method.__name__} –¥–ª—è {file_path}: {e}")
            
            # –ê–Ω–∞–ª–∏–∑ —Å Clang –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
            if CLANG_AVAILABLE:
                clang_issues = self._clang_analysis(file_path)
                issues.extend(clang_issues)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à
            self._cache[file_hash] = (issues, metrics)
            
            return issues, metrics
            
        except Exception as e:
            self.logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ {file_path}: {e}")
            return [], FileMetrics()

    def _pattern_based_analysis(self, file_path: Path, content: str) -> Tuple[List[CodeIssue], FileMetrics]:
        """–ê–Ω–∞–ª–∏–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        issues = []
        file_context = self._build_file_context(content)
        
        pattern_results = self.pattern_engine.analyze_with_context(content, file_context)
        
        for result in pattern_results:
            issue = CodeIssue(
                id=self._generate_issue_id(),
                file_path=str(file_path),
                line_number=self._find_line_number(content, result['position'][0]),
                column=0,
                issue_type=result['type'].upper(),
                category=AnalysisCategory.MEMORY_SAFETY,
                severity=IssueSeverity.HIGH if result['weight'] > 0.8 else IssueSeverity.MEDIUM,
                description=self._get_detailed_description(result['type']),
                code_snippet=result['match'][:200],
                confidence=result['weight'],
                suggestions=self._get_suggestions(result['type']),
                cwe=self._get_cwe_mappings(result['type'])
            )
            issues.append(issue)
        
        return issues, FileMetrics()

    def _control_flow_analysis(self, file_path: Path, content: str) -> Tuple[List[CodeIssue], FileMetrics]:
        """–ê–Ω–∞–ª–∏–∑ –ø–æ—Ç–æ–∫–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è"""
        issues = []
        
        cfg = self.control_flow_analyzer.build_control_flow_graph(content)
        data_flow = self.control_flow_analyzer.analyze_data_flow(cfg)
        
        # –ê–Ω–∞–ª–∏–∑ —É—Ç–µ—á–µ–∫ –ø–∞–º—è—Ç–∏ —á–µ—Ä–µ–∑ –ø–æ—Ç–æ–∫ –¥–∞–Ω–Ω—ã—Ö
        for var_name, state in data_flow.items():
            if state.get('allocated') and not state.get('freed'):
                issue = CodeIssue(
                    id=self._generate_issue_id(),
                    file_path=str(file_path),
                    line_number=state['allocation_line'],
                    column=0,
                    issue_type="DATAFLOW_MEMORY_LEAK",
                    category=AnalysisCategory.MEMORY_SAFETY,
                    severity=IssueSeverity.HIGH,
                    description=f"–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è —É—Ç–µ—á–∫–∞ –ø–∞–º—è—Ç–∏: '{var_name}' –≤—ã–¥–µ–ª–µ–Ω –Ω–æ –Ω–µ –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω",
                    code_snippet=f"–ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è: {var_name}",
                    confidence=0.8,
                    suggestions=["–î–æ–±–∞–≤–∏—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π free/delete", "–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≤—Å–µ –ø—É—Ç–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è"]
                )
                issues.append(issue)
        
        return issues, FileMetrics()

    def _semantic_analysis(self, file_path: Path, content: str) -> Tuple[List[CodeIssue], FileMetrics]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑"""
        issues = []
        
        symbols = self.semantic_analyzer.analyze_symbols(content)
        complexity_metrics = self.semantic_analyzer.analyze_function_complexity(content)
        
        # –ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π
        for func_name, metrics in complexity_metrics.items():
            if metrics['complexity'] > 10:
                issue = CodeIssue(
                    id=self._generate_issue_id(),
                    file_path=str(file_path),
                    line_number=1,  # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è
                    column=0,
                    issue_type="HIGH_COMPLEXITY",
                    category=AnalysisCategory.CODE_QUALITY,
                    severity=IssueSeverity.MEDIUM,
                    description=f"–§—É–Ω–∫—Ü–∏—è '{func_name}' –∏–º–µ–µ—Ç –≤—ã—Å–æ–∫—É—é —Ü–∏–∫–ª–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é —Å–ª–æ–∂–Ω–æ—Å—Ç—å ({metrics['complexity']})",
                    code_snippet=f"–§—É–Ω–∫—Ü–∏—è: {func_name}",
                    confidence=0.9,
                    suggestions=["–†–∞–∑–±–µ–π—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é –Ω–∞ –±–æ–ª–µ–µ –º–µ–ª–∫–∏–µ", "–£–ø—Ä–æ—Å—Ç–∏—Ç–µ –ª–æ–≥–∏–∫—É —É—Å–ª–æ–≤–∏—è"]
                )
                issues.append(issue)
        
        return issues, FileMetrics()

    def _metrics_analysis(self, file_path: Path, content: str) -> Tuple[List[CodeIssue], FileMetrics]:
        """–ê–Ω–∞–ª–∏–∑ –º–µ—Ç—Ä–∏–∫ –∫–æ–¥–∞"""
        metrics = FileMetrics()
        
        if RADON_AVAILABLE:
            try:
                # –ê–Ω–∞–ª–∏–∑ —Å—ã—Ä–æ–≥–æ –∫–æ–¥–∞
                raw_metrics = analyze(content)
                metrics.lines_of_code = raw_metrics.loc
                metrics.comment_ratio = raw_metrics.comments / raw_metrics.loc if raw_metrics.loc > 0 else 0
                
                # –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–æ—Å—Ç–∏
                mi_metrics = mi_visit(content, True)
                metrics.maintainability = mi_metrics
                
            except Exception as e:
                self.logger.warning(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫ –¥–ª—è {file_path}: {e}")
        
        return [], metrics

    def _security_analysis(self, file_path: Path, content: str) -> Tuple[List[CodeIssue], FileMetrics]:
        """–ê–Ω–∞–ª–∏–∑ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏"""
        issues = []
        
        security_patterns = {
            'buffer_overflow': [
                r'strcpy\s*\(\s*\w+\s*,\s*\w+\s*\)',
                r'strcat\s*\(\s*\w+\s*,\s*\w+\s*\)',
                r'sprintf\s*\(\s*\w+\s*,\s*[^)]*\)',
                r'gets\s*\(\s*\w+\s*\)'
            ],
            'format_string': [
                r'printf\s*\(\s*[^"]*\%[^"]*\)',
                r'sprintf\s*\(\s*\w+\s*,\s*[^"]*\%[^"]*\)'
            ],
            'command_injection': [
                r'system\s*\(',
                r'popen\s*\(',
                r'execl\s*\('
            ]
        }
        
        for vuln_type, patterns in security_patterns.items():
            for pattern in patterns:
                matches = re.finditer(pattern, content)
                for match in matches:
                    issue = CodeIssue(
                        id=self._generate_issue_id(),
                        file_path=str(file_path),
                        line_number=self._find_line_number(content, match.start()),
                        column=0,
                        issue_type=vuln_type.upper(),
                        category=AnalysisCategory.SECURITY,
                        severity=IssueSeverity.CRITICAL,
                        description=f"–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è —É—è–∑–≤–∏–º–æ—Å—Ç—å: {vuln_type}",
                        code_snippet=match.group(),
                        confidence=0.85,
                        suggestions=self._get_security_suggestions(vuln_type),
                        cwe=['CWE-120', 'CWE-134']  # Buffer Overflow, Format String
                    )
                    issues.append(issue)
        
        return issues, FileMetrics()

    def _performance_analysis(self, file_path: Path, content: str) -> Tuple[List[CodeIssue], FileMetrics]:
        """–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        issues = []
        
        performance_patterns = {
            'inefficient_loop': [
                r'for\s*\(\s*[^;]+;\s*[^;]+;\s*[^)]+\)\s*\{[^}]*strlen\s*\([^}]*\}',
                r'while\s*\([^)]*strlen'
            ],
            'redundant_copy': [
                r'memcpy\s*\(\s*\w+\s*,\s*\w+\s*,\s*strlen',
                r'strcpy\s*\(\s*\w+\s*,\s*\w+\s*\)\s*;\s*strcat'
            ]
        }
        
        for perf_issue, patterns in performance_patterns.items():
            for pattern in patterns:
                matches = re.finditer(pattern, content)
                for match in matches:
                    issue = CodeIssue(
                        id=self._generate_issue_id(),
                        file_path=str(file_path),
                        line_number=self._find_line_number(content, match.start()),
                        column=0,
                        issue_type=perf_issue.upper(),
                        category=AnalysisCategory.PERFORMANCE,
                        severity=IssueSeverity.MEDIUM,
                        description=f"–ü—Ä–æ–±–ª–µ–º–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {perf_issue}",
                        code_snippet=match.group(),
                        confidence=0.7,
                        suggestions=["–û–ø—Ç–∏–º–∏–∑–∏—Ä—É–π—Ç–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤ —Ü–∏–∫–ª–µ", "–ö—ç—à–∏—Ä—É–π—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–æ—Ä–æ–≥–∏—Ö –æ–ø–µ—Ä–∞—Ü–∏–π"]
                    )
                    issues.append(issue)
        
        return issues, FileMetrics()

    def _clang_analysis(self, file_path: Path) -> List[CodeIssue]:
        """–ê–Ω–∞–ª–∏–∑ —Å –ø–æ–º–æ—â—å—é Clang"""
        issues = []
        
        if not CLANG_AVAILABLE:
            return issues
        
        try:
            index = Index.create()
            tu = index.parse(str(file_path), args=['-std=c++11'])
            
            for diag in tu.diagnostics:
                issue = CodeIssue(
                    id=self._generate_issue_id(),
                    file_path=str(file_path),
                    line_number=diag.location.line,
                    column=diag.location.column,
                    issue_type="CLANG_" + diag.severity.name,
                    category=AnalysisCategory.CODE_QUALITY,
                    severity=IssueSeverity.MEDIUM,
                    description=diag.spelling,
                    code_snippet="",
                    confidence=0.9
                )
                issues.append(issue)
                
        except Exception as e:
            self.logger.warning(f"Clang –∞–Ω–∞–ª–∏–∑ –Ω–µ —É–¥–∞–ª—Å—è –¥–ª—è {file_path}: {e}")
        
        return issues

    def analyze_project(self) -> Dict[str, Any]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–µ–∫—Ç–∞"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss
        
        source_files = self.find_source_files()
        all_issues = []
        all_metrics = {}
        
        self.logger.info(f"–ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ {len(source_files)} —Ñ–∞–π–ª–æ–≤ —Å {self.max_processes} –ø—Ä–æ—Ü–µ—Å—Å–∞–º–∏...")
        
        # –ú–Ω–æ–≥–æ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        with ProcessPoolExecutor(max_workers=self.max_processes) as executor:
            future_to_file = {
                executor.submit(self.analyze_file_advanced, file): file 
                for file in source_files
            }
            
            for future in as_completed(future_to_file):
                file = future_to_file[future]
                try:
                    issues, metrics = future.result(timeout=300)  # 5 –º–∏–Ω—É—Ç —Ç–∞–π–º–∞—É—Ç
                    all_issues.extend(issues)
                    all_metrics[str(file)] = metrics
                    self.stats['files_analyzed'] += 1
                    
                    if self.stats['files_analyzed'] % 10 == 0:
                        self.logger.info(f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {self.stats['files_analyzed']}/{len(source_files)} —Ñ–∞–π–ª–æ–≤")
                        
                except Exception as e:
                    self.logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ {file}: {e}")
        
        self.issues = all_issues
        self.metrics = all_metrics
        self.stats['analysis_time'] = time.time() - start_time
        self.stats['memory_used'] = psutil.Process().memory_info().rss - start_memory
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫—ç—à–∞
        self._save_cache()
        
        return self.generate_comprehensive_report()

    def generate_comprehensive_report(self) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤—Å–µ–æ–±—ä–µ–º–ª—é—â–µ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        report = {
            'metadata': {
                'project_path': str(self.project_path),
                'analysis_timestamp': datetime.now().isoformat(),
                'analyzer_version': '2.0.0',
                'analysis_duration_seconds': self.stats['analysis_time'],
                'files_analyzed': self.stats['files_analyzed'],
                'total_issues_found': len(self.issues),
                'system_info': {
                    'platform': platform.platform(),
                    'processor': platform.processor(),
                    'python_version': platform.python_version(),
                    'memory_used_mb': self.stats['memory_used'] / 1024 / 1024
                }
            },
            'summary': {
                'issues_by_severity': defaultdict(int),
                'issues_by_category': defaultdict(int),
                'issues_by_type': defaultdict(int),
                'metrics_summary': self._calculate_metrics_summary()
            },
            'detailed_issues': [],
            'file_metrics': {},
            'recommendations': []
        }
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–±–ª–µ–º
        for issue in self.issues:
            report['summary']['issues_by_severity'][issue.severity.name] += 1
            report['summary']['issues_by_category'][issue.category.value] += 1
            report['summary']['issues_by_type'][issue.issue_type] += 1
            
            report['detailed_issues'].append({
                'id': issue.id,
                'file': issue.file_path,
                'line': issue.line_number,
                'column': issue.column,
                'type': issue.issue_type,
                'category': issue.category.value,
                'severity': issue.severity.name,
                'description': issue.description,
                'confidence': issue.confidence,
                'suggestions': issue.suggestions,
                'cwe': issue.cwe,
                'code_snippet': issue.code_snippet,
                'metrics': issue.metrics
            })
        
        # –ú–µ—Ç—Ä–∏–∫–∏ —Ñ–∞–π–ª–æ–≤
        for file_path, metrics in self.metrics.items():
            report['file_metrics'][file_path] = {
                'complexity': metrics.complexity,
                'maintainability': metrics.maintainability,
                'lines_of_code': metrics.lines_of_code,
                'comment_ratio': metrics.comment_ratio,
                'function_count': metrics.function_count,
                'issue_density': metrics.issue_density,
                'memory_risk_score': metrics.memory_risk_score
            }
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        report['recommendations'] = self._generate_recommendations()
        
        return report

    def _calculate_metrics_summary(self) -> Dict[str, Any]:
        """–†–∞—Å—á–µ—Ç —Å–≤–æ–¥–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫"""
        complexities = [m.complexity for m in self.metrics.values() if m.complexity > 0]
        maintainabilities = [m.maintainability for m in self.metrics.values() if m.maintainability > 0]
        comment_ratios = [m.comment_ratio for m in self.metrics.values() if m.comment_ratio > 0]

        return {
            'average_complexity': statistics.mean(complexities) if complexities else 0,
            'max_complexity': max(complexities) if complexities else 0,
            'average_maintainability': statistics.mean(maintainabilities) if maintainabilities else 0,
            'min_maintainability': min(maintainabilities) if maintainabilities else 0,
            'total_lines_of_code': sum(m.lines_of_code for m in self.metrics.values()),
            'average_comment_ratio': statistics.mean(comment_ratios) if comment_ratios else 0.0
        }

    def _generate_recommendations(self) -> List[Dict]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
        recommendations = []
        
        critical_issues = [i for i in self.issues if i.severity == IssueSeverity.CRITICAL]
        if critical_issues:
            recommendations.append({
                'priority': 'HIGH',
                'category': 'SECURITY',
                'description': f'–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(critical_issues)} –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏',
                'action': '–ù–µ–º–µ–¥–ª–µ–Ω–Ω–æ –∏—Å–ø—Ä–∞–≤—å—Ç–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ —É—è–∑–≤–∏–º–æ—Å—Ç–∏ –ø–µ—Ä–µ–¥ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ–º'
            })
        
        memory_issues = [i for i in self.issues if i.category == AnalysisCategory.MEMORY_SAFETY]
        if memory_issues:
            recommendations.append({
                'priority': 'HIGH',
                'category': 'MEMORY',
                'description': f'–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(memory_issues)} –ø—Ä–æ–±–ª–µ–º —Å –ø–∞–º—è—Ç—å—é',
                'action': '–ü—Ä–æ–≤–µ–¥–∏—Ç–µ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥ –∫–æ–¥–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞–º—è—Ç—å—é, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —É–º–Ω—ã–µ —É–∫–∞–∑–∞—Ç–µ–ª–∏'
            })
        
        # –ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        high_complex_files = [f for f, m in self.metrics.items() if m.complexity > 20]
        if high_complex_files:
            recommendations.append({
                'priority': 'MEDIUM',
                'category': 'MAINTAINABILITY',
                'description': f'–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(high_complex_files)} —Ñ–∞–π–ª–æ–≤ —Å –≤—ã—Å–æ–∫–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç—å—é',
                'action': '–£–ø—Ä–æ—Å—Ç–∏—Ç–µ —Å–ª–æ–∂–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏, —Ä–∞–∑–±–µ–π—Ç–µ –Ω–∞ –±–æ–ª–µ–µ –º–µ–ª–∫–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã'
            })
        
        return recommendations

    # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã
    def _build_file_context(self, content: str) -> Dict[str, Any]:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Ñ–∞–π–ª–∞"""
        lines = content.split('\n')
        return {
            'line_count': len(lines),
            'has_functions': bool(re.search(r'\w+\s+\w+\s*\([^)]*\)\s*\{', content)),
            'has_pointers': bool(re.search(r'\*', content)),
            'has_dynamic_allocation': bool(re.search(r'malloc|calloc|realloc|new', content)),
            'high_complexity': len(re.findall(r'if\s*\(|for\s*\(|while\s*\(|switch\s*\(', content)) > 10
        }

    def _find_line_number(self, content: str, position: int) -> int:
        """–ù–∞—Ö–æ–∂–¥–µ–Ω–∏–µ –Ω–æ–º–µ—Ä–∞ —Å—Ç—Ä–æ–∫–∏ –ø–æ –ø–æ–∑–∏—Ü–∏–∏ –≤ —Ç–µ–∫—Å—Ç–µ"""
        return content[:position].count('\n') + 1

    def _generate_issue_id(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ ID –ø—Ä–æ–±–ª–µ–º—ã"""
        return hashlib.md5(f"{time.time()}{len(self.issues)}".encode()).hexdigest()[:8]

    def _get_detailed_description(self, issue_type: str) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã"""
        descriptions = {
            'memory_leak': '–£—Ç–µ—á–∫–∞ –ø–∞–º—è—Ç–∏: –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –≤—ã–¥–µ–ª–µ–Ω–Ω–∞—è –ø–∞–º—è—Ç—å –Ω–µ –æ—Å–≤–æ–±–æ–∂–¥–∞–µ—Ç—Å—è',
            'double_free': '–î–≤–æ–π–Ω–æ–µ –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ: –ø–æ–ø—ã—Ç–∫–∞ –æ—Å–≤–æ–±–æ–¥–∏—Ç—å —É–∂–µ –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–Ω—É—é –ø–∞–º—è—Ç—å',
            'use_after_free': '–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è: –¥–æ—Å—Ç—É–ø –∫ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ free/delete',
            'buffer_overflow': '–ü–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏–µ –±—É—Ñ–µ—Ä–∞: –∑–∞–ø–∏—Å—å –∑–∞ –ø—Ä–µ–¥–µ–ª—ã –≤—ã–¥–µ–ª–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏',
            'null_pointer': '–†–∞–∑—ã–º–µ–Ω–æ–≤–∞–Ω–∏–µ –Ω—É–ª–µ–≤–æ–≥–æ —É–∫–∞–∑–∞—Ç–µ–ª—è',
            'high_complexity': '–í—ã—Å–æ–∫–∞—è —Ü–∏–∫–ª–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å —Ñ—É–Ω–∫—Ü–∏–∏'
        }
        return descriptions.get(issue_type, '–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞')

    def _get_suggestions(self, issue_type: str) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—é"""
        suggestions = {
            'memory_leak': [
                '–î–æ–±–∞–≤—å—Ç–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –≤—ã–∑–æ–≤ free() –∏–ª–∏ delete',
                '–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —É–º–Ω—ã–µ —É–∫–∞–∑–∞—Ç–µ–ª–∏ –≤ C++',
                '–†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ RAII'
            ],
            'double_free': [
                '–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ —É–∫–∞–∑–∞—Ç–µ–ª—å –≤ NULL –ø–æ—Å–ª–µ –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è',
                '–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥–µ—Ç–µ–∫—Ç–æ—Ä –¥–≤–æ–π–Ω–æ–≥–æ –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è',
                '–†–µ–æ—Ä–≥–∞–Ω–∏–∑—É–π—Ç–µ –ª–æ–≥–∏–∫—É –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è –ø–∞–º—è—Ç–∏'
            ],
            'use_after_free': [
                '–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ —É–∫–∞–∑–∞—Ç–µ–ª—å –≤ NULL –ø–æ—Å–ª–µ –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è',
                '–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –ø–∞–º—è—Ç–∏ (Valgrind, ASan)',
                '–ü–µ—Ä–µ—Å–º–æ—Ç—Ä–∏—Ç–µ –≤—Ä–µ–º—è –∂–∏–∑–Ω–∏ –æ–±—ä–µ–∫—Ç–æ–≤'
            ]
        }
        return suggestions.get(issue_type, ['–ü—Ä–æ–≤–µ–¥–∏—Ç–µ —Ä—É—á–Ω–æ–π –∞—É–¥–∏—Ç –∫–æ–¥–∞'])

    def _get_cwe_mappings(self, issue_type: str) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π CWE"""
        cwe_map = {
            'memory_leak': ['CWE-401'],
            'double_free': ['CWE-415'],
            'use_after_free': ['CWE-416'],
            'buffer_overflow': ['CWE-120', 'CWE-119'],
            'null_pointer': ['CWE-476']
        }
        return cwe_map.get(issue_type, [])

    def _get_security_suggestions(self, vuln_type: str) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏"""
        suggestions = {
            'buffer_overflow': [
                '–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ strncpy –≤–º–µ—Å—Ç–æ strcpy',
                '–î–æ–±–∞–≤—å—Ç–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –≥—Ä–∞–Ω–∏—Ü –±—É—Ñ–µ—Ä–æ–≤',
                '–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ —Å—Ç—Ä–æ–∫–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏'
            ],
            'format_string': [
                '–ù–∏–∫–æ–≥–¥–∞ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –≤–≤–æ–¥ –∫–∞–∫ —Å—Ç—Ä–æ–∫—É —Ñ–æ—Ä–º–∞—Ç–∞',
                '–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ —Ñ–æ—Ä–º–∞—Ç–∞',
                '–í–∞–ª–∏–¥–∏—Ä—É–π—Ç–µ –≤—Å–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ'
            ]
        }
        return suggestions.get(vuln_type, ['–û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞–º –ø–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ–º—É –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é'])

    def _merge_metrics(self, target: FileMetrics, source: FileMetrics):
        """–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫"""
        for attr in ['complexity', 'maintainability', 'lines_of_code', 
                    'comment_ratio', 'function_count', 'issue_density', 'memory_risk_score']:
            if getattr(source, attr) > 0:
                setattr(target, attr, getattr(source, attr))

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ —É–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
    parser = argparse.ArgumentParser(
        description='ULTIMATE C/C++ Static Analyzer - Senior Level',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  python ultimate_analyzer.py /path/to/project --processes 8 --verbose
  python ultimate_analyzer.py /path/to/project --format json --output detailed_report
  python ultimate_analyzer.py /path/to/project --force-reanalyze --security-scan
        """
    )
    
    parser.add_argument('project_path', help='–ü—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞')
    parser.add_argument('--processes', type=int, default=mp.cpu_count(), 
                       help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞')
    parser.add_argument('--format', choices=['json', 'html', 'both'], default='both',
                       help='–§–æ—Ä–º–∞—Ç –æ—Ç—á–µ—Ç–∞')
    parser.add_argument('--output', help='–ò–º—è —Ñ–∞–π–ª–∞ –æ—Ç—á–µ—Ç–∞ (–±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è)')
    parser.add_argument('--verbose', action='store_true', help='–ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥')
    parser.add_argument('--force-reanalyze', action='store_true', 
                       help='–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –ø–µ—Ä–µ–∞–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤')
    parser.add_argument('--security-scan', action='store_true',
                       help='–£–≥–ª—É–±–ª–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏')
    parser.add_argument('--performance-scan', action='store_true',
                       help='–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏')
    
    args = parser.parse_args()
    
    if not os.path.exists(args.project_path):
        logging.error(f"–ü—É—Ç—å –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {args.project_path}")
        return
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
    config = {
        'max_processes': args.processes,
        'verbose': args.verbose,
        'force_reanalyze': args.force_reanalyze,
        'security_scan': args.security_scan,
        'performance_scan': args.performance_scan
    }
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
    analyzer = AdvancedMemoryAnalyzer(args.project_path, config)
    
    logging.info("üöÄ –ó–∞–ø—É—Å–∫ ULTIMATE C/C++ Static Analyzer...")
    logging.info(f"üìä –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ—Ü–µ—Å—Å–æ–≤: {args.processes}")
    logging.info(f"üîç –ê–Ω–∞–ª–∏–∑ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏: {'–í–ö–õ' if args.security_scan else '–í–´–ö–õ'}")
    logging.info(f"‚ö° –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {'–í–ö–õ' if args.performance_scan else '–í–´–ö–õ'}")
    
    report = analyzer.analyze_project()
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–æ–≤
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_name = args.output or f"ultimate_analysis_{timestamp}"
    
    if args.format in ['json', 'both']:
        json_file = Path(args.project_path) / f"{output_name}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        logging.info(f"üìÑ JSON –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {json_file}")
    
    if args.format in ['html', 'both']:
        html_file = Path(args.project_path) / f"{output_name}.html"
        generate_html_report(report, html_file)
        logging.info(f"üåê HTML –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {html_file}")
    
    # –í—ã–≤–æ–¥ –∏—Ç–æ–≥–æ–≤–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    logging.info("=" * 80)
    logging.info("üéâ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù!")
    logging.info("=" * 80)
    logging.info(f"üìÅ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {report['metadata']['files_analyzed']}")
    logging.info(f"‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º: {report['metadata']['total_issues_found']}")
    logging.info(f"‚è±Ô∏è  –í—Ä–µ–º—è –∞–Ω–∞–ª–∏–∑–∞: {report['metadata']['analysis_duration_seconds']:.2f} —Å–µ–∫")
    logging.info(f"üíæ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –ø–∞–º—è—Ç–∏: {report['metadata']['system_info']['memory_used_mb']:.2f} MB")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏
    severity_stats = report['summary']['issues_by_severity']
    for severity, count in severity_stats.items():
        logging.info(f"   {severity}: {count}")
    
    if report['metadata']['total_issues_found'] > 0:
        logging.info("üî¥ –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã! –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ.")
    else:
        logging.info("‚úÖ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ.")

def generate_html_report(report: Dict, output_file: Path):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è HTML –æ—Ç—á–µ—Ç–∞"""
    html_template = """
    <!DOCTYPE html>
    <html lang="ru">
    <head>
        <meta charset="UTF-8">
        <title>ULTIMATE Analysis Report</title>
        <style>
            body { font-family: Arial, sans-serif; margin: 40px; }
            .header { background: #2c3e50; color: white; padding: 20px; border-radius: 5px; }
            .metric { background: #ecf0f1; padding: 15px; margin: 10px 0; border-radius: 5px; }
            .issue { border-left: 4px solid #e74c3c; padding: 10px; margin: 5px 0; background: #fdf2f2; }
            .critical { border-color: #c0392b; }
            .high { border-color: #e74c3c; }
            .medium { border-color: #f39c12; }
            .low { border-color: #f1c40f; }
            .info { border-color: #3498db; }
        </style>
    </head>
    <body>
        <div class="header">
            <h1>üöÄ ULTIMATE C/C++ Static Analysis Report</h1>
            <p>Generated: {{timestamp}}</p>
        </div>
        
        <div class="metric">
            <h2>üìä Summary</h2>
            <p>Files Analyzed: {{files_analyzed}}</p>
            <p>Total Issues: {{total_issues}}</p>
            <p>Analysis Time: {{analysis_time}} seconds</p>
        </div>
        
        <h2>‚ö†Ô∏è Issues by Severity</h2>
        {% for severity, count in severity_stats.items() %}
        <div class="metric">
            <strong>{{ severity }}:</strong> {{ count }}
        </div>
        {% endfor %}
        
        <h2>üîç Detailed Issues</h2>
        {% for issue in issues %}
        <div class="issue {{ issue.severity.lower() }}">
            <strong>{{ issue.file }}:{{ issue.line }}</strong> [{{ issue.severity }}]
            <br>{{ issue.description }}
            <br><small>{{ issue.code_snippet }}</small>
        </div>
        {% endfor %}
    </body>
    </html>
    """
    
    # –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —à–∞–±–ª–æ–Ω–∏–∑–∞—Ç–æ—Ä–∞
    html_content = html_template.replace('{{timestamp}}', report['metadata']['analysis_timestamp'])
    html_content = html_content.replace('{{files_analyzed}}', str(report['metadata']['files_analyzed']))
    html_content = html_content.replace('{{total_issues}}', str(report['metadata']['total_issues_found']))
    html_content = html_content.replace('{{analysis_time}}', str(report['metadata']['analysis_duration_seconds']))
    
    # –ó–∞–º–µ–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏
    severity_html = ""
    for severity, count in report['summary']['issues_by_severity'].items():
        severity_html += f'<div class="metric"><strong>{severity}:</strong> {count}</div>\n'
    html_content = html_content.replace('{% for severity, count in severity_stats.items() %}', severity_html)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(html_content)

if __name__ == "__main__":
    main()
  